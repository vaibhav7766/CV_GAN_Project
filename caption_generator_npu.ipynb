{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Girish\\.conda\\envs\\venv\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Girish\\.conda\\envs\\venv\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import intel_npu_acceleration_library\n",
    "from intel_npu_acceleration_library.compiler import CompilerConfig\n",
    "from intel_npu_acceleration_library.dtypes import int8\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image Prompt Dataset\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, captions_file):\n",
    "        with open(captions_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.image_filenames = list(self.data.keys())\n",
    "        self.captions = list(self.data.values())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        caption = self.captions[idx]\n",
    "        return image_filename, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "captions_file = r\"D:\\Work\\College\\img_prompts_part_2.json\"\n",
    "result_file = r\"D:\\Work\\College\\merged_captions_part_2.json\"\n",
    "if not os.path.exists(result_file):    \n",
    "    with open(result_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        ...\n",
    "dataset = CaptionDataset(captions_file)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girish\\AppData\\Local\\Temp\\ipykernel_19824\\2754742975.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"nemotron_model_npu.pth\")\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "# tokenizer_name = \"nvidia/Nemotron-Mini-4B-Instruct\"\n",
    "tokenizer_name = r\"D:\\Work\\College\\nemotron_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_default_system_prompt=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# model_name = \"nvidia/Nemotron-Mini-4B-Instruct\"\n",
    "# model_name = \"nemotron_model\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).eval()\n",
    "# model = intel_npu_acceleration_library.compile(model, CompilerConfig(dtype=int8))\n",
    "\n",
    "\n",
    "# Save model and tokenizer\n",
    "# tokenizer.save_pretrained(\"nemotron_tokenizer\")\n",
    "# model.save_pretrained(\"nemotron_model\")\n",
    "# torch.save(model, \"nemotron_model.pth\")\n",
    "model = torch.load(\"nemotron_model_npu.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(id):\n",
    "    pth, prompt = dataset[id]\n",
    "\n",
    "    # prompt = (\n",
    "    #     \"The following captions describe the same image in different ways. \"\n",
    "    #     \"Merge them into a single, clear, and accurate description of the same scene, avoiding repetition:\\n\\n\"\n",
    "    #     + \"\\n\".join(lst)\n",
    "    #     + \"\\n\\nMerged Paragraph:\"\n",
    "    # )\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output\n",
    "    with torch.inference_mode():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and clean up output\n",
    "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    generated_paragraph = output_text.split(\"Merged Paragraph:\")[-1].strip()\n",
    "    \n",
    "    return pth, generated_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4160\n"
     ]
    }
   ],
   "source": [
    "with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    checkpoint = len(f.readlines()) - 1\n",
    "\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Captions: 100%|██████████| 5000/5000 [3:08:39<00:00,  2.26s/it]  \n"
     ]
    }
   ],
   "source": [
    "with open(result_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    if checkpoint == -1:\n",
    "      f.write(\"{\\n\")  # Start JSON object\n",
    "      first_entry = True\n",
    "    else:\n",
    "      first_entry = False\n",
    "\n",
    "    for i in tqdm(range(5000), total=5000, desc=\"Generating Captions\"):\n",
    "        if i < checkpoint:\n",
    "            continue\n",
    "\n",
    "        img, generated_caption = generate_caption(i)\n",
    "\n",
    "        # Write each entry separately\n",
    "        if not first_entry:\n",
    "          f.write(\",\\n\")  # Add a comma before new entries (except the first)\n",
    "        json.dump(img, f)\n",
    "        f.write(\": \")\n",
    "        json.dump(generated_caption, f)\n",
    "\n",
    "        f.flush()  # Ensure data is written immediately\n",
    "        first_entry = False\n",
    "\n",
    "    f.write(\"\\n}\")  # End JSON object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
