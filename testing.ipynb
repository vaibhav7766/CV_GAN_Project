{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:17.985983Z",
     "iopub.status.busy": "2025-03-15T17:10:17.985591Z",
     "iopub.status.idle": "2025-03-15T17:10:18.001129Z",
     "shell.execute_reply": "2025-03-15T17:10:18.000217Z",
     "shell.execute_reply.started": "2025-03-15T17:10:17.985943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.002385Z",
     "iopub.status.busy": "2025-03-15T17:10:18.002151Z",
     "iopub.status.idle": "2025-03-15T17:10:18.027533Z",
     "shell.execute_reply": "2025-03-15T17:10:18.026722Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.002362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_dim = 32  # Dimension for CNN encoder output and transformer decoder\n",
    "num_layers = 1  # Number of layers in the transformer decoder\n",
    "nhead = 2  # Number of attention heads in the transformer decoder\n",
    "dim_feedforward = 32  # Dimension of the feedforward network in the transformer decoder\n",
    "batch_size = 8  # Batch size for data loaders\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "num_epochs = 20  # Number of epochs for training\n",
    "max_length = 50  # Maximum length for captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.030368Z",
     "iopub.status.busy": "2025-03-15T17:10:18.030020Z",
     "iopub.status.idle": "2025-03-15T17:10:18.038472Z",
     "shell.execute_reply": "2025-03-15T17:10:18.037772Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.030330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Dataset Definition ---\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading images and captions.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, captions_file, tokenizer, max_length=max_length):\n",
    "        self.image_dir = image_dir\n",
    "        with open(captions_file, \"r\") as f:\n",
    "            self.data = json.load(f)  # Expected format: {\"image1.jpg\": \"caption\", ...}\n",
    "        self.image_filenames = list(self.data.keys())\n",
    "        self.captions = list(self.data.values())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[idx]\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return (\n",
    "            image,\n",
    "            tokenized.input_ids.squeeze(),\n",
    "            tokenized.attention_mask.squeeze(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.039646Z",
     "iopub.status.busy": "2025-03-15T17:10:18.039382Z",
     "iopub.status.idle": "2025-03-15T17:10:18.052377Z",
     "shell.execute_reply": "2025-03-15T17:10:18.051569Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.039622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- CNN Encoder ---\n",
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\"Encoder using EfficientNet-B0 to extract image features.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=feature_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "        self.features = efficientnet.features  # Extract up to last conv layer\n",
    "        self.projection = nn.Conv2d(1280, feature_dim, kernel_size=1, stride=1)\n",
    "        self.feature_dim = feature_dim  # Store feature_dim as an instance variable\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.features(images)  # Shape: (batch_size, 1280, 7, 7)\n",
    "        features = self.projection(features)  # Shape: (batch_size, feature_dim, 7, 7)\n",
    "        batch_size = features.size(0)\n",
    "        features = features.permute(0, 2, 3, 1).reshape(\n",
    "            batch_size, 49, self.feature_dim\n",
    "        )\n",
    "        return features  # Shape: (batch_size, 49, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.053630Z",
     "iopub.status.busy": "2025-03-15T17:10:18.053358Z",
     "iopub.status.idle": "2025-03-15T17:10:18.068288Z",
     "shell.execute_reply": "2025-03-15T17:10:18.067371Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.053606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Transformer Decoder ---\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"Lightweight Transformer decoder for caption generation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        nhead=nhead,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "    ):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=feature_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=False,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(feature_dim, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_features, captions):\n",
    "        embeddings = self.embedding(captions).permute(\n",
    "            1, 0, 2\n",
    "        )  # (seq_len, batch_size, feature_dim)\n",
    "        memory = encoder_features.permute(1, 0, 2)  # (49, batch_size, feature_dim)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(captions.size(1)).to(\n",
    "            captions.device\n",
    "        )\n",
    "        output = self.decoder(embeddings, memory, tgt_mask=tgt_mask)\n",
    "        logits = self.fc(output.permute(1, 0, 2))  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# --- Combined Image Captioning Model ---\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    \"\"\"Combines CNN encoder and Transformer decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.069566Z",
     "iopub.status.busy": "2025-03-15T17:10:18.069279Z",
     "iopub.status.idle": "2025-03-15T17:10:18.081711Z",
     "shell.execute_reply": "2025-03-15T17:10:18.081087Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.069542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Validation Function ---\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Compute validation loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, _ in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            images, input_ids = images.to(device), input_ids.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(images, input_ids[:, :-1])\n",
    "                loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.size(-1)), input_ids[:, 1:].reshape(-1)\n",
    "                )\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.082968Z",
     "iopub.status.busy": "2025-03-15T17:10:18.082716Z",
     "iopub.status.idle": "2025-03-15T17:10:18.098708Z",
     "shell.execute_reply": "2025-03-15T17:10:18.097995Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.082944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    model_save_path,\n",
    "    patience=3,\n",
    "    min_delta=0.001,\n",
    "):\n",
    "    \"\"\"Train the model in full precision with early stopping.\"\"\"\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    with open(\"training_results.csv\", \"w\", newline=\"\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"best_val_loss\", \"lr\"])\n",
    "        csv_file.flush()\n",
    "\n",
    "        # Outer tqdm loop for epochs\n",
    "        with tqdm(range(num_epochs), desc=\"Training Progress\") as epoch_pbar:\n",
    "            for epoch in epoch_pbar:\n",
    "                model.train()\n",
    "                total_loss = 0.0\n",
    "                # Inner tqdm loop for batches, completes before outer bar updates\n",
    "                for images, input_ids, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "                    images, input_ids = images.to(device), input_ids.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images, input_ids[:, :-1])\n",
    "                    loss = criterion(\n",
    "                        outputs.reshape(-1, outputs.size(-1)), input_ids[:, 1:].reshape(-1)\n",
    "                    )\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                # Compute metrics after the full epoch\n",
    "                avg_train_loss = total_loss / len(train_loader)\n",
    "                avg_val_loss = validate_model(model, val_loader, criterion, device)\n",
    "                scheduler.step(avg_val_loss)\n",
    "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                # Update postfix only after the epoch completes\n",
    "                epoch_pbar.set_postfix({\n",
    "                    \"Train Loss\": f\"{avg_train_loss:.4f}\",\n",
    "                    \"Val Loss\": f\"{avg_val_loss:.4f}\",\n",
    "                    \"Best Val Loss\": f\"{best_loss:.4f}\",\n",
    "                    \"LR\": f\"{current_lr:.6f}\"\n",
    "                })\n",
    "\n",
    "                # Check for improvement and save model\n",
    "                if best_loss - avg_val_loss > min_delta:\n",
    "                    best_loss = avg_val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    print(f\"Best model saved with val loss {best_loss:.4f}\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                csv_writer.writerow(\n",
    "                    [epoch + 1, avg_train_loss, avg_val_loss, best_loss, current_lr]\n",
    "                )\n",
    "                csv_file.flush()\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.099864Z",
     "iopub.status.busy": "2025-03-15T17:10:18.099602Z",
     "iopub.status.idle": "2025-03-15T17:10:18.111382Z",
     "shell.execute_reply": "2025-03-15T17:10:18.110761Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.099822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Caption Generation ---\n",
    "def generate_caption(model, image, tokenizer, max_length=max_length, device=\"cuda\"):\n",
    "    \"\"\"Generate a caption for a single image.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        features = model.encoder(image)  # (1, 49, feature_dim)\n",
    "        generated = torch.tensor([tokenizer.bos_token_id], device=device).unsqueeze(0)\n",
    "        for _ in range(max_length - 1):\n",
    "            logits = model.decoder(features, generated)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "        return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.113416Z",
     "iopub.status.busy": "2025-03-15T17:10:18.113139Z",
     "iopub.status.idle": "2025-03-15T17:10:18.132549Z",
     "shell.execute_reply": "2025-03-15T17:10:18.131697Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.113392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Evaluation Function ---\n",
    "def evaluate_model(model, test_loader, tokenizer, device):\n",
    "    \"\"\"Evaluate the model using BLEU, CIDEr, METEOR, ROUGE-L, and BERT scores.\"\"\"\n",
    "    model.eval()\n",
    "    refs, hyps = {}, {}\n",
    "    with torch.no_grad():\n",
    "        for i, (images, input_ids, _) in enumerate(test_loader):\n",
    "            generated = [\n",
    "                generate_caption(model, img, tokenizer, device=device) for img in images\n",
    "            ]\n",
    "            references = [\n",
    "                tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids\n",
    "            ]\n",
    "            for j, (ref, hyp) in enumerate(zip(references, generated)):\n",
    "                idx = i * test_loader.batch_size + j\n",
    "                refs[idx] = [ref]\n",
    "                hyps[idx] = [hyp]\n",
    "\n",
    "    # CIDEr\n",
    "    cider_scorer = Cider()\n",
    "    cider_score, _ = cider_scorer.compute_score(refs, hyps)\n",
    "\n",
    "    # METEOR\n",
    "    meteor_scorer = Meteor()\n",
    "    meteor_score, _ = meteor_scorer.compute_score(refs, hyps)\n",
    "\n",
    "    # ROUGE-L\n",
    "    rouge_scorer = Rouge()\n",
    "    rouge_score, _ = rouge_scorer.compute_score(refs, hyps)\n",
    "\n",
    "    # BERT Score\n",
    "    ref_list = [r[0] for r in refs.values()]\n",
    "    hyp_list = [h[0] for h in hyps.values()]\n",
    "    P, R, F1 = bert_score(hyp_list, ref_list, lang=\"en\", verbose=True)\n",
    "    bert_f1 = F1.mean().item()\n",
    "\n",
    "    # BLEU\n",
    "    bleu_score = corpus_bleu(\n",
    "        [[r.split()] for r in ref_list], [h.split() for h in hyp_list]\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"BLEU: {bleu_score:.4f}, CIDEr: {cider_score:.4f}, METEOR: {meteor_score:.4f}, \"\n",
    "        f\"ROUGE-L: {rouge_score:.4f}, BERT Score: {bert_f1:.4f}\"\n",
    "    )\n",
    "    return bleu_score, cider_score, meteor_score, rouge_score, bert_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:18.133698Z",
     "iopub.status.busy": "2025-03-15T17:10:18.133486Z",
     "iopub.status.idle": "2025-03-15T17:10:19.858742Z",
     "shell.execute_reply": "2025-03-15T17:10:19.857853Z",
     "shell.execute_reply.started": "2025-03-15T17:10:18.133677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "IMAGE_DIR = \"train2017_20k_processed\"  # Replace with your image directory\n",
    "CAPTIONS_FILE = \"merged_captions.json\"  # Replace with your captions JSON file\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:19.859993Z",
     "iopub.status.busy": "2025-03-15T17:10:19.859742Z",
     "iopub.status.idle": "2025-03-15T17:10:19.970976Z",
     "shell.execute_reply": "2025-03-15T17:10:19.970290Z",
     "shell.execute_reply.started": "2025-03-15T17:10:19.859970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoaders\n",
    "dataset = ImageCaptionDataset(\n",
    "    IMAGE_DIR, CAPTIONS_FILE, tokenizer, max_length=max_length\n",
    ")\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:19.972464Z",
     "iopub.status.busy": "2025-03-15T17:10:19.972184Z",
     "iopub.status.idle": "2025-03-15T17:10:21.239520Z",
     "shell.execute_reply": "2025-03-15T17:10:21.238612Z",
     "shell.execute_reply.started": "2025-03-15T17:10:19.972439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Python\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "vocab_size = tokenizer.vocab_size\n",
    "encoder = CNNEncoder(feature_dim=feature_dim)\n",
    "decoder = TransformerDecoder(\n",
    "    feature_dim=feature_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    ")\n",
    "model = ImageCaptioningModel(encoder, decoder).to(device)\n",
    "\n",
    "# Training Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-15T17:10:21.240939Z",
     "iopub.status.busy": "2025-03-15T17:10:21.240585Z",
     "iopub.status.idle": "2025-03-15T18:07:51.136270Z",
     "shell.execute_reply": "2025-03-15T18:07:51.135149Z",
     "shell.execute_reply.started": "2025-03-15T17:10:21.240897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    model_save_path=\"my_model.pth\",\n",
    "    patience=3,\n",
    "    min_delta=0.001,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:07:51.138476Z",
     "iopub.status.busy": "2025-03-15T18:07:51.138131Z",
     "iopub.status.idle": "2025-03-15T18:18:05.028545Z",
     "shell.execute_reply": "2025-03-15T18:18:05.027423Z",
     "shell.execute_reply.started": "2025-03-15T18:07:51.138440Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/552299279.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"my_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d649a6575f342ce8e37028fecf6bc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b7def4f45c447b86a3b3c26d6faf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938d226beb91420cbe9279f34f50c519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70715ef15ba945378f708dc625995142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef9b497b4744ba3ad66a1eb20f48e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a09dceaadf44d9f9a796731e67cf1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c961eae66b94c4a8c6771c6675eb857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886e9f655d004865a6ff83b1cf2eeb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 25.07 seconds, 119.65 sentences/sec\n",
      "BLEU: 0.0240, CIDEr: 0.0622, METEOR: 0.1259, ROUGE-L: 0.1938, BERT Score: 0.8458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02400378036626487,\n",
       " 0.062165027043621306,\n",
       " 0.12588561906337187,\n",
       " 0.19379292239336057,\n",
       " 0.8457581400871277)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Best Model and Evaluate\n",
    "model.load_state_dict(torch.load(\"my_model.pth\"))\n",
    "evaluate_model(model, test_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6774132,
     "sourceId": 10899842,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6864720,
     "sourceId": 11023690,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
