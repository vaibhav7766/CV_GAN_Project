{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "feature_dim = 256  # Increased dimension for richer features\n",
    "num_layers = 6     # More layers for deeper processing\n",
    "nhead = 8          # More attention heads for multi-faceted attention\n",
    "dim_feedforward = 1024  # Larger feedforward network\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Vocabulary Size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.bos_token = tokenizer.cls_token  # Use [CLS] as <BOS>\n",
    "tokenizer.eos_token = tokenizer.sep_token  # Use [SEP] as <EOS>\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"BERT Vocabulary Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Definition ---\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_file, tokenizer, max_length=max_length):\n",
    "        self.image_dir = image_dir\n",
    "        with open(captions_file, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.image_filenames = list(self.data.keys())\n",
    "        self.captions = list(self.data.values())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[idx]\n",
    "        tokenized = self.tokenizer(\n",
    "            caption,\n",
    "            add_special_tokens=True,  # Adds [CLS] and [SEP]\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "        return image, input_ids, attention_mask, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced CNN Encoder ---\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim=feature_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(2048, 1024, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, feature_dim, kernel_size=1, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.attn_pool = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=4, batch_first=True)\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.features(images)\n",
    "        features = self.projection(features)\n",
    "        batch_size = features.size(0)\n",
    "        features = features.permute(0, 2, 3, 1).reshape(batch_size, 49, self.feature_dim)\n",
    "        attn_output, _ = self.attn_pool(features, features, features)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced Transformer Decoder ---\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, num_layers=num_layers, nhead=nhead, dim_feedforward=dim_feedforward):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_length, feature_dim))\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=feature_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=0.1, batch_first=False\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(feature_dim * 2, vocab_size)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "\n",
    "    def forward(self, encoder_features, captions, tgt_key_padding_mask=None):\n",
    "        batch_size = captions.size(0)\n",
    "        embeddings = self.embedding(captions) + self.pos_encoder[:, :captions.size(1), :]\n",
    "        embeddings = self.norm(embeddings).permute(1, 0, 2)\n",
    "        memory = encoder_features.permute(1, 0, 2)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(captions.size(1)).to(captions.device)\n",
    "        output = self.decoder(embeddings, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        output = self.norm(output)\n",
    "        logits = self.fc(output.permute(1, 0, 2))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combined Model ---\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions, tgt_key_padding_mask=None):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validation Function ---\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, _ in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(images, input_ids[:, :-1], tgt_key_padding_mask=(attention_mask[:, :-1] == 0))\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), input_ids[:, 1:].reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, model_save_path):\n",
    "    scaler = GradScaler()\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    patience = 3\n",
    "\n",
    "    with open(\"training_results.csv\", \"w\", newline=\"\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"best_val_loss\", \"lr\"])\n",
    "        csv_file.flush()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for images, input_ids, attention_mask, _ in train_loader:\n",
    "                images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    outputs = model(images, input_ids[:, :-1], tgt_key_padding_mask=(attention_mask[:, :-1] == 0))\n",
    "                    loss = criterion(outputs.reshape(-1, outputs.size(-1)), input_ids[:, 1:].reshape(-1))\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "                epoch_pbar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "                epoch_pbar.update(1)\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            avg_val_loss = validate_model(model, val_loader, criterion, device)\n",
    "            scheduler.step(avg_val_loss)\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            epoch_pbar.set_postfix({\n",
    "                \"Train Loss\": f\"{avg_train_loss:.4f}\",\n",
    "                \"Val Loss\": f\"{avg_val_loss:.4f}\",\n",
    "                \"Best Val Loss\": f\"{best_loss:.4f}\",\n",
    "                \"LR\": f\"{current_lr:.6f}\"\n",
    "            })\n",
    "            epoch_pbar.close()\n",
    "\n",
    "            if best_loss - avg_val_loss > 0.001:\n",
    "                best_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"Best model saved with val loss {best_loss:.4f}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            csv_writer.writerow([epoch + 1, avg_train_loss, avg_val_loss, best_loss, current_lr])\n",
    "            csv_file.flush()\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Caption Generation ---\n",
    "def generate_caption(model, image, tokenizer, max_length=max_length, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        features = model.encoder(image)\n",
    "        generated = torch.tensor([tokenizer.bos_token_id], device=device).unsqueeze(0)\n",
    "        caption_ids = [tokenizer.bos_token_id]\n",
    "        for _ in range(max_length - 1):\n",
    "            logits = model.decoder(features, generated)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            token_id = next_token.item()\n",
    "            caption_ids.append(token_id)\n",
    "            if token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "        caption = tokenizer.decode(caption_ids, skip_special_tokens=True)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Function ---\n",
    "def evaluate_model(model, test_loader, tokenizer, device):\n",
    "    model.eval()\n",
    "    refs, hyps = {}, {}\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _, _, captions) in enumerate(test_loader):\n",
    "            generated = [generate_caption(model, img, tokenizer, device=device) for img in images]\n",
    "            for j, (ref, hyp) in enumerate(zip(captions, generated)):\n",
    "                idx = i * test_loader.batch_size + j\n",
    "                refs[idx] = [ref]\n",
    "                hyps[idx] = [hyp]\n",
    "\n",
    "    cider_scorer = Cider()\n",
    "    cider_score, _ = cider_scorer.compute_score(refs, hyps)\n",
    "    meteor_scorer = Meteor()\n",
    "    meteor_score, _ = meteor_scorer.compute_score(refs, hyps)\n",
    "    rouge_scorer = Rouge()\n",
    "    rouge_score, _ = rouge_scorer.compute_score(refs, hyps)\n",
    "    ref_list = [r[0] for r in refs.values()]\n",
    "    hyp_list = [h[0] for h in hyps.values()]\n",
    "    P, R, F1 = bert_score(hyp_list, ref_list, lang=\"en\", verbose=True)\n",
    "    bert_f1 = F1.mean().item()\n",
    "    bleu_score = corpus_bleu([[r.split()] for r in ref_list], [h.split() for h in hyp_list])\n",
    "\n",
    "    print(\n",
    "        f\"BLEU: {bleu_score:.4f}, CIDEr: {cider_score:.4f}, METEOR: {meteor_score:.4f}, \"\n",
    "        f\"ROUGE-L: {rouge_score:.4f}, BERT Score: {bert_f1:.4f}\"\n",
    "    )\n",
    "    return bleu_score, cider_score, meteor_score, rouge_score, bert_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Function for Specific Image ---\n",
    "def test_image_caption(image_path, model, tokenizer, device):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    caption = generate_caption(model, image, tokenizer, max_length=max_length, device=device)\n",
    "    print(f\"Generated Caption for {image_path}: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = r\"/media/vaibhav/Programming/Project/train2017\"\n",
    "CAPTIONS_FILE = \"merged_captions.json\"\n",
    "MODEL_SAVE_PATH = \"complex_model_bert.pth\"\n",
    "\n",
    "# Dataset and DataLoaders\n",
    "dataset = ImageCaptionDataset(IMAGE_DIR, CAPTIONS_FILE, tokenizer, max_length=max_length)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vaibhav/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "encoder = CNNEncoder(feature_dim=feature_dim)\n",
    "decoder = TransformerDecoder(\n",
    "    feature_dim=feature_dim, vocab_size=vocab_size, num_layers=num_layers,\n",
    "    nhead=nhead, dim_feedforward=dim_feedforward\n",
    ")\n",
    "model = ImageCaptioningModel(encoder, decoder).to(device)\n",
    "\n",
    "# Training Setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3418/2060057057.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/50:   0%|          | 0/1750 [00:00<?, ?it/s]/tmp/ipykernel_3418/2060057057.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/vaibhav/.local/lib/python3.10/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/50: 100%|██████████| 1750/1750 [02:46<00:00, 10.47it/s, Batch Loss=3.8516]/tmp/ipykernel_3418/2535806672.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/50: 100%|██████████| 1750/1750 [02:56<00:00,  9.89it/s, Train Loss=4.3830, Val Loss=3.6167, Best Val Loss=inf, LR=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with val loss 3.6167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:   1%|▏         | 22/1750 [00:02<02:50, 10.14it/s, Batch Loss=3.4599]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, model_save_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     26\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 27\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m epoch_pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     29\u001b[0m epoch_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "    num_epochs, device, MODEL_SAVE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:14:57.033123: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 14:14:57.154894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742892297.199837    3418 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742892297.213063    3418 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742892297.307862    3418 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742892297.307881    3418 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742892297.307882    3418 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742892297.307883    3418 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-25 14:14:57.320692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Epoch 2/50:   1%|▏         | 23/1750 [00:12<02:50, 10.14it/s, Batch Loss=3.4599]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:18<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 114.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 18.50 seconds, 162.20 sentences/sec\n",
      "BLEU: 0.0218, CIDEr: 0.0745, METEOR: 0.0938, ROUGE-L: 0.1968, BERT Score: 0.8721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.021847830750659554,\n",
       " np.float64(0.07453058917477384),\n",
       " 0.09382001402752198,\n",
       " np.float64(0.19684828735403861),\n",
       " 0.8721246719360352)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Best Model and Evaluate\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "evaluate_model(model, test_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption for /media/vaibhav/Programming/Project/train2017/000000391895.jpg: a group of people are seen walking down a street, with a large city street.\n"
     ]
    }
   ],
   "source": [
    "# Test on a specific image\n",
    "test_image_path = r\"/media/vaibhav/Programming/Project/train2017/000000391895.jpg\"\n",
    "test_image_caption(test_image_path, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
